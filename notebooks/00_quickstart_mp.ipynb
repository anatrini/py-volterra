{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Memory Polynomial (MP) Models\n",
    "\n",
    "This notebook demonstrates the basics of **Memory Polynomial (MP)** models for nonlinear system identification using the `py-volterra` library.\n",
    "\n",
    "## What is a Memory Polynomial?\n",
    "\n",
    "A Memory Polynomial is a **diagonal Volterra model** that captures nonlinear behavior with memory effects. For a single-input single-output (SISO) system:\n",
    "\n",
    "$$\n",
    "y(t) = \\sum_{n=1}^{N} \\sum_{m=0}^{M-1} h_n[m] \\cdot x^n(t-m)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the **nonlinearity order** (e.g., 3 for cubic nonlinearity)\n",
    "- $M$ is the **memory length** (number of past samples)\n",
    "- $h_n[m]$ are the **Volterra kernels** (model parameters)\n",
    "- $x(t)$ is the input signal\n",
    "- $y(t)$ is the output signal\n",
    "\n",
    "**Key properties:**\n",
    "- **Diagonal structure**: Only same-order terms (e.g., $x^2(t) \\cdot x^2(t-1)$ is excluded)\n",
    "- **Efficient**: $O(M \\cdot N)$ parameters instead of $O(N^M)$\n",
    "- **Common applications**: Power amplifiers, audio saturation, weakly nonlinear systems\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "# Import py-volterra\n",
    "from volterra import GeneralizedMemoryPolynomial\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generate Synthetic Nonlinear Data\n",
    "\n",
    "We'll create a synthetic system with:\n",
    "- **Linear term**: $0.8 \\cdot x(t)$\n",
    "- **Quadratic term**: $0.15 \\cdot x^2(t)$\n",
    "- **Cubic term**: $0.05 \\cdot x^3(t)$\n",
    "- **Memory effects**: 2nd-order IIR filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input signal: bandlimited Gaussian noise\n",
    "fs = 48000  # Sampling rate (Hz)\n",
    "duration = 0.5  # seconds\n",
    "n_samples = int(fs * duration)\n",
    "\n",
    "# Create bandlimited noise (100 Hz - 8 kHz)\n",
    "x_white = np.random.randn(n_samples)\n",
    "sos = signal.butter(6, [100, 8000], btype='bandpass', fs=fs, output='sos')\n",
    "x = signal.sosfilt(sos, x_white)\n",
    "x = x / np.std(x) * 0.3  # Normalize to RMS = 0.3\n",
    "\n",
    "# True nonlinear system (ground truth)\n",
    "def true_system(x):\n",
    "    \"\"\"Nonlinear system with memory: cascaded nonlinearity + IIR filter.\"\"\"\n",
    "    # Static nonlinearity\n",
    "    y_nl = 0.8 * x + 0.15 * x**2 + 0.05 * x**3\n",
    "    \n",
    "    # Memory (2nd-order IIR lowpass filter)\n",
    "    b = [0.2, -0.38, 0.18]  # Numerator coefficients\n",
    "    a = [1.0, -1.9, 0.94]   # Denominator coefficients\n",
    "    y = signal.lfilter(b, a, y_nl)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Generate output with added noise\n",
    "y_clean = true_system(x)\n",
    "noise = np.random.randn(n_samples) * 0.01  # SNR ≈ 30 dB\n",
    "y = y_clean + noise\n",
    "\n",
    "print(f\"Generated {n_samples} samples ({duration} seconds at {fs} Hz)\")\n",
    "print(f\"Input RMS: {np.sqrt(np.mean(x**2)):.4f}\")\n",
    "print(f\"Output RMS: {np.sqrt(np.mean(y**2)):.4f}\")\n",
    "print(f\"SNR: {10 * np.log10(np.mean(y_clean**2) / np.mean(noise**2)):.1f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input and output signals\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Time domain (first 1000 samples)\n",
    "t_ms = np.arange(1000) / fs * 1000  # Convert to milliseconds\n",
    "axes[0].plot(t_ms, x[:1000], label='Input x(t)', alpha=0.7)\n",
    "axes[0].plot(t_ms, y[:1000], label='Output y(t)', alpha=0.7)\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Time Domain Signals')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Input/Output characteristic\n",
    "axes[1].scatter(x[::10], y[::10], alpha=0.3, s=1)\n",
    "axes[1].set_xlabel('Input x(t)')\n",
    "axes[1].set_ylabel('Output y(t)')\n",
    "axes[1].set_title('Nonlinear I/O Characteristic')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Frequency domain\n",
    "f_x, Pxx = signal.welch(x, fs=fs, nperseg=2048)\n",
    "f_y, Pyy = signal.welch(y, fs=fs, nperseg=2048)\n",
    "axes[2].semilogy(f_x / 1000, Pxx, label='Input', alpha=0.7)\n",
    "axes[2].semilogy(f_y / 1000, Pyy, label='Output', alpha=0.7)\n",
    "axes[2].set_xlabel('Frequency (kHz)')\n",
    "axes[2].set_ylabel('PSD (V²/Hz)')\n",
    "axes[2].set_title('Power Spectral Density')\n",
    "axes[2].set_xlim([0, 10])\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Fit a Memory Polynomial Model\n",
    "\n",
    "We'll fit an MP model with:\n",
    "- **Memory length** `M = 10` (captures past 10 samples)\n",
    "- **Nonlinearity order** `N = 3` (linear + quadratic + cubic terms)\n",
    "- **Regularization** `lambda_reg = 1e-6` (prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% training, 30% testing\n",
    "n_train = int(0.7 * n_samples)\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit Memory Polynomial model\n",
    "mp_model = GeneralizedMemoryPolynomial(\n",
    "    memory_length=10,\n",
    "    order=3,\n",
    "    lags=None,  # None = diagonal MP (all lags for each order)\n",
    "    lambda_reg=1e-6\n",
    ")\n",
    "\n",
    "# Fit the model (least-squares estimation)\n",
    "mp_model.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Model fitted successfully!\")\n",
    "print(f\"Number of parameters: {mp_model.memory_length * mp_model.order}\")\n",
    "print(f\"Coefficient matrix shape: {mp_model.coeffs_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evaluate Model Performance\n",
    "\n",
    "We'll evaluate using:\n",
    "- **NMSE** (Normalized Mean Squared Error): $\\text{NMSE} = \\frac{\\|y - \\hat{y}\\|^2}{\\|y\\|^2}$\n",
    "- **Visual comparison** of predicted vs. true output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and test sets\n",
    "y_train_pred = mp_model.predict(x_train)\n",
    "y_test_pred = mp_model.predict(x_test)\n",
    "\n",
    "# Note: predictions have length T - memory_length + 1 due to memory effects\n",
    "# Trim ground truth to match\n",
    "M = mp_model.memory_length\n",
    "y_train_trimmed = y_train[M - 1:]\n",
    "y_test_trimmed = y_test[M - 1:]\n",
    "\n",
    "# Compute NMSE (Normalized Mean Squared Error)\n",
    "def compute_nmse(y_true, y_pred):\n",
    "    \"\"\"Compute NMSE in dB.\"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    signal_power = np.mean(y_true ** 2)\n",
    "    nmse = mse / signal_power\n",
    "    nmse_db = 10 * np.log10(nmse)\n",
    "    return nmse_db\n",
    "\n",
    "nmse_train = compute_nmse(y_train_trimmed, y_train_pred)\n",
    "nmse_test = compute_nmse(y_test_trimmed, y_test_pred)\n",
    "\n",
    "print(f\"Training NMSE: {nmse_train:.2f} dB\")\n",
    "print(f\"Test NMSE: {nmse_test:.2f} dB\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  NMSE < -20 dB: Excellent fit\")\n",
    "print(f\"  NMSE < -10 dB: Good fit\")\n",
    "print(f\"  NMSE > -10 dB: Poor fit (underfitting or wrong model structure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# Test set: time domain comparison\n",
    "n_plot = 1000\n",
    "t_ms = np.arange(n_plot) / fs * 1000\n",
    "axes[0, 0].plot(t_ms, y_test_trimmed[:n_plot], label='True output', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].plot(t_ms, y_test_pred[:n_plot], label='MP prediction', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[0, 0].set_xlabel('Time (ms)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].set_title(f'Test Set Prediction (NMSE: {nmse_test:.2f} dB)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set: prediction error\n",
    "error = y_test_trimmed[:n_plot] - y_test_pred[:n_plot]\n",
    "axes[0, 1].plot(t_ms, error, alpha=0.7, linewidth=1, color='red')\n",
    "axes[0, 1].axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0, 1].set_xlabel('Time (ms)')\n",
    "axes[0, 1].set_ylabel('Error')\n",
    "axes[0, 1].set_title(f'Prediction Error (RMS: {np.sqrt(np.mean(error**2)):.4f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set: scatter plot (predicted vs. true)\n",
    "axes[1, 0].scatter(y_test_trimmed[::10], y_test_pred[::10], alpha=0.3, s=2)\n",
    "y_range = [y_test_trimmed.min(), y_test_trimmed.max()]\n",
    "axes[1, 0].plot(y_range, y_range, 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1, 0].set_xlabel('True output')\n",
    "axes[1, 0].set_ylabel('Predicted output')\n",
    "axes[1, 0].set_title('Predicted vs. True (Test Set)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axis('equal')\n",
    "\n",
    "# Error histogram\n",
    "full_error = y_test_trimmed - y_test_pred\n",
    "axes[1, 1].hist(full_error, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Prediction Error')\n",
    "axes[1, 1].set_ylabel('Probability Density')\n",
    "axes[1, 1].set_title(f'Error Distribution (μ={np.mean(full_error):.2e}, σ={np.std(full_error):.4f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualize Learned Kernels\n",
    "\n",
    "The Memory Polynomial learns **Volterra kernels** $h_n[m]$ for each nonlinearity order $n$ and memory tap $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned kernels\n",
    "kernels = mp_model.coeffs_  # Shape: (memory_length, order)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for n in range(mp_model.order):\n",
    "    h_n = kernels[:, n]  # Kernel for order n+1\n",
    "    \n",
    "    axes[n].stem(range(mp_model.memory_length), h_n, basefmt=' ')\n",
    "    axes[n].set_xlabel('Memory tap m')\n",
    "    axes[n].set_ylabel(f'h_{n+1}[m]')\n",
    "    axes[n].set_title(f'Order {n+1} Kernel ({"Linear" if n == 0 else "Quadratic" if n == 1 else "Cubic"})')\n",
    "    axes[n].grid(True, alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Kernel interpretation:\")\n",
    "print(\"  - Linear kernel (order 1): Captures linear memory effects (similar to FIR filter)\")\n",
    "print(\"  - Quadratic kernel (order 2): Captures even-order harmonics and memory\")\n",
    "print(\"  - Cubic kernel (order 3): Captures odd-order harmonics and memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Order Selection\n",
    "\n",
    "How do we choose the right `memory_length` and `order`? Let's compare different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different model orders\n",
    "orders = [1, 2, 3, 5]\n",
    "memory_lengths = [3, 5, 10, 20]\n",
    "\n",
    "results = []\n",
    "\n",
    "for order in orders:\n",
    "    for mem_len in memory_lengths:\n",
    "        # Fit model\n",
    "        model = GeneralizedMemoryPolynomial(\n",
    "            memory_length=mem_len,\n",
    "            order=order,\n",
    "            lags=None,\n",
    "            lambda_reg=1e-6\n",
    "        )\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_test_trim = y_test[mem_len - 1:]\n",
    "        nmse = compute_nmse(y_test_trim, y_pred)\n",
    "        n_params = mem_len * order\n",
    "        \n",
    "        results.append({\n",
    "            'order': order,\n",
    "            'memory_length': mem_len,\n",
    "            'n_params': n_params,\n",
    "            'nmse_db': nmse\n",
    "        })\n",
    "\n",
    "# Convert to structured array for plotting\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Plot NMSE vs. model complexity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# NMSE vs. number of parameters\n",
    "for order in orders:\n",
    "    df_order = df[df['order'] == order]\n",
    "    axes[0].plot(df_order['n_params'], df_order['nmse_db'], 'o-', label=f'Order {order}', markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('Number of Parameters')\n",
    "axes[0].set_ylabel('Test NMSE (dB)')\n",
    "axes[0].set_title('Model Complexity vs. Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(-20, color='green', linestyle='--', linewidth=1, alpha=0.5, label='Excellent threshold')\n",
    "\n",
    "# Heatmap: NMSE vs. order and memory length\n",
    "pivot = df.pivot(index='order', columns='memory_length', values='nmse_db')\n",
    "im = axes[1].imshow(pivot.values, aspect='auto', cmap='RdYlGn_r', vmin=-30, vmax=-10)\n",
    "axes[1].set_xticks(range(len(memory_lengths)))\n",
    "axes[1].set_xticklabels(memory_lengths)\n",
    "axes[1].set_yticks(range(len(orders)))\n",
    "axes[1].set_yticklabels(orders)\n",
    "axes[1].set_xlabel('Memory Length')\n",
    "axes[1].set_ylabel('Nonlinearity Order')\n",
    "axes[1].set_title('Test NMSE (dB) Heatmap')\n",
    "plt.colorbar(im, ax=axes[1], label='NMSE (dB)')\n",
    "\n",
    "# Annotate heatmap with values\n",
    "for i, order in enumerate(orders):\n",
    "    for j, mem_len in enumerate(memory_lengths):\n",
    "        text = axes[1].text(j, i, f\"{pivot.iloc[i, j]:.1f}\",\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  - Higher order improves fit for nonlinear systems\")\n",
    "print(\"  - Longer memory captures more dynamic effects\")\n",
    "print(\"  - Diminishing returns: too many parameters → overfitting risk\")\n",
    "print(\"\\nBest configuration (lowest NMSE):\")\n",
    "best_idx = df['nmse_db'].idxmin()\n",
    "best = df.loc[best_idx]\n",
    "print(f\"  Order: {best['order']}, Memory: {best['memory_length']}, NMSE: {best['nmse_db']:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Generated synthetic data** from a nonlinear system with memory\n",
    "2. **Fitted a Memory Polynomial model** using least-squares\n",
    "3. **Evaluated performance** using NMSE and visual inspection\n",
    "4. **Visualized learned kernels** to understand model behavior\n",
    "5. **Compared different model orders** to guide hyperparameter selection\n",
    "\n",
    "### When to use Memory Polynomial models:\n",
    "- ✅ **Weakly nonlinear systems** (power amplifiers, audio saturation)\n",
    "- ✅ **Diagonal Volterra assumption holds** (no cross-memory interactions)\n",
    "- ✅ **Fast training required** (linear least-squares)\n",
    "- ❌ **Strong cross-memory effects** → use GMP (Generalized Memory Polynomial)\n",
    "- ❌ **High-dimensional MIMO** → use TT-Volterra (Tensor-Train decomposition)\n",
    "\n",
    "### Next steps:\n",
    "- **Notebook 01**: Generalized Memory Polynomial (GMP) for cross-term modeling\n",
    "- **Notebook 02**: Tensor-Train Volterra for full MIMO systems\n",
    "- **Notebook 03**: Automatic model selection (MP vs GMP vs TT-Full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
