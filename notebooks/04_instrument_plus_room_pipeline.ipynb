{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Application: Instrument + Room Acoustic Pipeline\n",
    "\n",
    "This notebook demonstrates a **cascaded Volterra system** for modeling a complete audio chain:\n",
    "\n",
    "$$\n",
    "\\text{Input signal} \\xrightarrow{\\text{Instrument}} \\text{Nonlinear output} \\xrightarrow{\\text{Room}} \\text{Final recording}\n",
    "$$\n",
    "\n",
    "## Application: Electric Guitar + Amplifier + Room\n",
    "\n",
    "We'll model a complete signal chain:\n",
    "1. **Instrument stage**: Guitar → nonlinear saturation/distortion (MP or GMP)\n",
    "2. **Room stage**: Acoustic response with reverb/reflections (linear convolution)\n",
    "\n",
    "**Why Volterra?**\n",
    "- Captures harmonic distortion from tube amplifiers\n",
    "- Models speaker nonlinearities (magnetic saturation)\n",
    "- Handles memory effects (bias drift, thermal dynamics)\n",
    "\n",
    "**Use cases:**\n",
    "- Digital audio effects (amp simulators, distortion pedals)\n",
    "- Virtual acoustics (room emulation)\n",
    "- Audio forensics (device fingerprinting)\n",
    "- Hearing aid nonlinearity compensation\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "from volterra import AcousticChain, GeneralizedMemoryPolynomial\n",
    "\n",
    "np.random.seed(1011)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generate Synthetic Instrument + Room System\n",
    "\n",
    "We'll create:\n",
    "- **Instrument**: Nonlinear saturation (cubic soft-clipper)\n",
    "- **Room**: Linear impulse response (exponentially decaying reverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "fs = 48000  # Sampling rate\n",
    "duration = 2.0  # seconds\n",
    "n_samples = int(fs * duration)\n",
    "\n",
    "# Generate input signal: guitar-like plucked tone\n",
    "def generate_guitar_signal(fs, duration, f0=220):\n",
    "    \"\"\"\n",
    "    Simulate a guitar pluck using Karplus-Strong algorithm.\n",
    "    \"\"\"\n",
    "    t = np.arange(int(fs * duration)) / fs\n",
    "    \n",
    "    # Initial excitation: burst of noise\n",
    "    excitation_length = int(fs / f0)  # One period\n",
    "    excitation = np.random.randn(excitation_length) * 0.8\n",
    "    excitation = np.concatenate([excitation, np.zeros(len(t) - excitation_length)])\n",
    "    \n",
    "    # Feedback filter (lowpass for realistic decay)\n",
    "    delay = int(fs / f0)\n",
    "    y = np.zeros_like(t)\n",
    "    y[:delay] = excitation[:delay]\n",
    "    \n",
    "    alpha = 0.995  # Decay factor\n",
    "    for i in range(delay, len(t)):\n",
    "        y[i] = alpha * 0.5 * (y[i - delay] + y[i - delay - 1]) + excitation[i]\n",
    "    \n",
    "    # Add some harmonic content\n",
    "    y += 0.2 * np.sin(2 * np.pi * f0 * t) * np.exp(-3 * t)\n",
    "    y += 0.1 * np.sin(2 * np.pi * 2 * f0 * t) * np.exp(-4 * t)\n",
    "    \n",
    "    return y / np.max(np.abs(y)) * 0.6  # Normalize\n",
    "\n",
    "x = generate_guitar_signal(fs, duration, f0=220)  # A3 note (220 Hz)\n",
    "\n",
    "print(f\"Generated guitar signal: {len(x)} samples ({duration}s at {fs} Hz)\")\n",
    "print(f\"RMS: {np.sqrt(np.mean(x**2)):.4f}\")\n",
    "print(f\"Peak: {np.max(np.abs(x)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instrument nonlinearity (amp distortion)\n",
    "def instrument_distortion(x):\n",
    "    \"\"\"\n",
    "    Tube amplifier-like soft saturation:\n",
    "    - Linear at low levels\n",
    "    - Soft clipping at high levels (tanh-like)\n",
    "    - Adds harmonics\n",
    "    \"\"\"\n",
    "    # Polynomial approximation of soft saturation\n",
    "    gain = 1.5  # Drive\n",
    "    x_driven = x * gain\n",
    "    \n",
    "    y_nl = (\n",
    "        0.85 * x_driven +           # Linear (fundamental)\n",
    "        0.12 * x_driven**2 +        # Even harmonics (2nd)\n",
    "        -0.08 * x_driven**3         # Odd harmonics (3rd) + soft clipping\n",
    "    )\n",
    "    \n",
    "    # Add memory via simple IIR (speaker resonance)\n",
    "    b = [0.25, -0.45, 0.22]\n",
    "    a = [1.0, -1.88, 0.92]\n",
    "    y = signal.lfilter(b, a, y_nl)\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Define room impulse response (exponential decay reverb)\n",
    "def generate_room_ir(fs, duration=0.8, rt60=0.3):\n",
    "    \"\"\"\n",
    "    Generate a synthetic room impulse response.\n",
    "    RT60: reverberation time (time for 60 dB decay)\n",
    "    \"\"\"\n",
    "    n_samples = int(fs * duration)\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Exponential decay envelope\n",
    "    decay_const = 3 * np.log(10) / rt60  # For 60 dB = factor of 1000\n",
    "    envelope = np.exp(-decay_const * t)\n",
    "    \n",
    "    # Add some early reflections (sparse peaks)\n",
    "    ir = envelope * np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Direct sound (delta at t=0)\n",
    "    ir[0] += 1.0\n",
    "    \n",
    "    # Early reflections (first 50ms)\n",
    "    early_times = [0.008, 0.015, 0.023, 0.035, 0.048]  # seconds\n",
    "    for t_reflect in early_times:\n",
    "        idx = int(t_reflect * fs)\n",
    "        if idx < len(ir):\n",
    "            ir[idx] += np.random.uniform(0.2, 0.5) * np.exp(-decay_const * t_reflect)\n",
    "    \n",
    "    return ir / np.max(np.abs(ir))  # Normalize\n",
    "\n",
    "room_ir = generate_room_ir(fs, duration=0.8, rt60=0.3)\n",
    "\n",
    "print(f\"\\nRoom impulse response: {len(room_ir)} samples ({len(room_ir)/fs:.3f}s)\")\n",
    "print(f\"RT60: 0.3 seconds (typical small room)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full chain: input → instrument → room → output\n",
    "y_instrument = instrument_distortion(x)\n",
    "y_full = signal.fftconvolve(y_instrument, room_ir, mode='same')\n",
    "\n",
    "# Add measurement noise\n",
    "y_full += np.random.randn(n_samples) * 0.005\n",
    "\n",
    "print(f\"Output signal RMS: {np.sqrt(np.mean(y_full**2)):.4f}\")\n",
    "print(f\"SNR: ~{20 * np.log10(np.sqrt(np.mean(y_full**2)) / 0.005):.1f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the acoustic chain\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\n",
    "# Time-domain signals\n",
    "n_plot = int(0.5 * fs)  # First 0.5 seconds\n",
    "t_plot = np.arange(n_plot) / fs\n",
    "\n",
    "axes[0, 0].plot(t_plot, x[:n_plot], alpha=0.7, linewidth=1)\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].set_title('1. Input: Guitar Pluck')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(t_plot, y_instrument[:n_plot], alpha=0.7, linewidth=1, color='orange')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Amplitude')\n",
    "axes[0, 1].set_title('2. After Instrument (nonlinear distortion)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(t_plot, y_full[:n_plot], alpha=0.7, linewidth=1, color='green')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Amplitude')\n",
    "axes[1, 0].set_title('3. Final Output (after room)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Room impulse response\n",
    "t_ir = np.arange(len(room_ir)) / fs * 1000  # milliseconds\n",
    "axes[1, 1].plot(t_ir, room_ir, alpha=0.7, linewidth=1, color='red')\n",
    "axes[1, 1].set_xlabel('Time (ms)')\n",
    "axes[1, 1].set_ylabel('Amplitude')\n",
    "axes[1, 1].set_title('Room Impulse Response')\n",
    "axes[1, 1].set_xlim([0, 300])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral analysis: compare input, instrument, and room outputs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, signal_data, title in zip(axes, [x, y_instrument, y_full], \n",
    "                                   ['Input', 'After Instrument', 'Final Output']):\n",
    "    f, Pxx = signal.welch(signal_data, fs=fs, nperseg=4096)\n",
    "    ax.semilogy(f / 1000, Pxx, alpha=0.7, linewidth=1.5)\n",
    "    ax.set_xlabel('Frequency (kHz)')\n",
    "    ax.set_ylabel('PSD (V²/Hz)')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim([0, 5])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark fundamental and harmonics\n",
    "    f0 = 220  # Hz\n",
    "    for harmonic in [1, 2, 3, 4]:\n",
    "        ax.axvline(harmonic * f0 / 1000, color='red', linestyle='--', \n",
    "                   linewidth=0.8, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Spectral Evolution Through Acoustic Chain (harmonics marked in red)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the added harmonics from instrument distortion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Model the Acoustic Chain with Volterra\n",
    "\n",
    "We'll use `AcousticChain` to jointly identify:\n",
    "- **Instrument model**: Nonlinear (MP or GMP)\n",
    "- **Room model**: Linear FIR filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "n_train = int(0.7 * n_samples)\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y_full[:n_train], y_full[n_train:]\n",
    "\n",
    "print(f\"Training samples: {n_train} ({n_train/fs:.2f}s)\")\n",
    "print(f\"Testing samples: {len(x_test)} ({len(x_test)/fs:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AcousticChain model\n",
    "chain = AcousticChain(\n",
    "    instrument_memory=8,      # Memory length for instrument nonlinearity\n",
    "    instrument_order=3,       # Nonlinearity order (cubic)\n",
    "    room_ir_length=2048,      # Room impulse response length (42.7 ms at 48 kHz)\n",
    "    lambda_reg=1e-5           # Regularization\n",
    ")\n",
    "\n",
    "print(\"AcousticChain configuration:\")\n",
    "print(f\"  Instrument: Memory={chain.instrument_memory}, Order={chain.instrument_order}\")\n",
    "print(f\"  Room IR length: {chain.room_ir_length} samples ({chain.room_ir_length/fs*1000:.1f} ms)\")\n",
    "print(f\"  Regularization: {chain.lambda_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the acoustic chain\n",
    "import time\n",
    "\n",
    "print(\"Fitting acoustic chain model...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "chain.fit(x_train, y_train)\n",
    "\n",
    "fit_time = time.time() - start\n",
    "\n",
    "print(f\"\\nModel fitted in {fit_time:.2f} seconds\")\n",
    "print(f\"Instrument model parameters: {chain.instrument_model.coeffs_.size}\")\n",
    "print(f\"Room IR parameters: {len(chain.room_ir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_test_pred = chain.predict(x_test)\n",
    "\n",
    "# Note: prediction length accounting for both instrument and room memory\n",
    "total_memory = chain.instrument_memory + chain.room_ir_length - 1\n",
    "y_test_trimmed = y_test[total_memory - 1:len(y_test_pred) + total_memory - 1]\n",
    "\n",
    "print(f\"Prediction shape: {y_test_pred.shape}\")\n",
    "print(f\"Ground truth (trimmed) shape: {y_test_trimmed.shape}\")\n",
    "\n",
    "# Compute NMSE\n",
    "mse = np.mean((y_test_trimmed - y_test_pred) ** 2)\n",
    "signal_power = np.mean(y_test_trimmed ** 2)\n",
    "nmse_db = 10 * np.log10(mse / signal_power)\n",
    "\n",
    "print(f\"\\nTest NMSE: {nmse_db:.2f} dB\")\n",
    "print(f\"Test MSE: {mse:.6f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if nmse_db < -20:\n",
    "    print(\"  ✅ Excellent fit! Model captured instrument + room dynamics\")\n",
    "elif nmse_db < -10:\n",
    "    print(\"  ⚠️  Good fit, but some details missing (try higher order or longer memory)\")\n",
    "else:\n",
    "    print(\"  ❌ Poor fit (check model configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test set predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\n",
    "n_plot = int(0.4 * fs)\n",
    "t_plot = np.arange(n_plot) / fs\n",
    "\n",
    "# Time-domain comparison\n",
    "axes[0, 0].plot(t_plot, y_test_trimmed[:n_plot], label='True output', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].plot(t_plot, y_test_pred[:n_plot], label='Chain prediction', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].set_title(f'Time Domain Prediction (NMSE: {nmse_db:.2f} dB)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction error\n",
    "error = y_test_trimmed[:n_plot] - y_test_pred[:n_plot]\n",
    "axes[0, 1].plot(t_plot, error, alpha=0.7, color='red')\n",
    "axes[0, 1].axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Error')\n",
    "axes[0, 1].set_title(f'Prediction Error (RMS: {np.sqrt(mse):.5f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Spectral comparison\n",
    "f, Pyy_true = signal.welch(y_test_trimmed, fs=fs, nperseg=4096)\n",
    "f, Pyy_pred = signal.welch(y_test_pred, fs=fs, nperseg=4096)\n",
    "axes[1, 0].semilogy(f / 1000, Pyy_true, label='True', alpha=0.7, linewidth=1.5)\n",
    "axes[1, 0].semilogy(f / 1000, Pyy_pred, label='Predicted', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[1, 0].set_xlabel('Frequency (kHz)')\n",
    "axes[1, 0].set_ylabel('PSD (V²/Hz)')\n",
    "axes[1, 0].set_title('Spectral Comparison')\n",
    "axes[1, 0].set_xlim([0, 4])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1, 1].scatter(y_test_trimmed[::20], y_test_pred[::20], alpha=0.3, s=2)\n",
    "y_range = [y_test_trimmed.min(), y_test_trimmed.max()]\n",
    "axes[1, 1].plot(y_range, y_range, 'r--', linewidth=2, label='Perfect')\n",
    "axes[1, 1].set_xlabel('True output')\n",
    "axes[1, 1].set_ylabel('Predicted output')\n",
    "axes[1, 1].set_title('Predicted vs. True')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Analyze Learned Components\n",
    "\n",
    "Let's examine what the model learned about the instrument and room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned instrument kernels\n",
    "instrument_kernels = chain.instrument_model.coeffs_\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for n in range(chain.instrument_order):\n",
    "    h_n = instrument_kernels[:, n]\n",
    "    \n",
    "    axes[n].stem(range(len(h_n)), h_n, basefmt=' ')\n",
    "    axes[n].set_xlabel('Memory lag')\n",
    "    axes[n].set_ylabel(f'h_{n+1}[m]')\n",
    "    axes[n].set_title(f'Instrument Kernel: Order {n+1}')\n",
    "    axes[n].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Learned Instrument Nonlinearity', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Instrument kernel interpretation:\")\n",
    "print(\"  - Order 1 (linear): Main signal path + frequency shaping\")\n",
    "print(\"  - Order 2 (quadratic): Even harmonics (2nd, 4th, ...)\")\n",
    "print(\"  - Order 3 (cubic): Odd harmonics (3rd, 5th, ...) + soft clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned room IR\n",
    "learned_room_ir = chain.room_ir\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Time domain comparison\n",
    "t_ir = np.arange(len(room_ir)) / fs * 1000\n",
    "t_learned = np.arange(len(learned_room_ir)) / fs * 1000\n",
    "\n",
    "axes[0].plot(t_ir, room_ir, label='True room IR', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(t_learned[:len(room_ir)], learned_room_ir[:len(room_ir)], \n",
    "             label='Learned room IR', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Room Impulse Response: Time Domain')\n",
    "axes[0].set_xlim([0, 300])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Frequency domain comparison\n",
    "H_true = np.fft.rfft(room_ir, n=8192)\n",
    "H_learned = np.fft.rfft(learned_room_ir, n=8192)\n",
    "freqs = np.fft.rfftfreq(8192, 1/fs)\n",
    "\n",
    "axes[1].semilogx(freqs[1:] / 1000, 20 * np.log10(np.abs(H_true[1:])), \n",
    "                 label='True', alpha=0.7, linewidth=1.5)\n",
    "axes[1].semilogx(freqs[1:] / 1000, 20 * np.log10(np.abs(H_learned[1:])), \n",
    "                 label='Learned', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[1].set_xlabel('Frequency (kHz)')\n",
    "axes[1].set_ylabel('Magnitude (dB)')\n",
    "axes[1].set_title('Room Response: Frequency Domain')\n",
    "axes[1].set_xlim([0.1, 10])\n",
    "axes[1].set_ylim([-40, 5])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRoom IR analysis:\")\n",
    "print(f\"  True IR length: {len(room_ir)} samples\")\n",
    "print(f\"  Learned IR length: {len(learned_room_ir)} samples\")\n",
    "print(f\"  Correlation: {np.corrcoef(room_ir[:min(len(room_ir), len(learned_room_ir))], learned_room_ir[:min(len(room_ir), len(learned_room_ir))])[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Application: Real-Time Processing\n",
    "\n",
    "Once trained, the model can be used for real-time audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time processing with block-based prediction\n",
    "block_size = 512  # typical audio buffer size\n",
    "n_blocks = len(x_test) // block_size\n",
    "\n",
    "print(f\"Simulating real-time processing:\")\n",
    "print(f\"  Block size: {block_size} samples ({block_size/fs*1000:.2f} ms latency)\")\n",
    "print(f\"  Number of blocks: {n_blocks}\")\n",
    "\n",
    "# Process block-by-block\n",
    "y_realtime = []\n",
    "processing_times = []\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    start_idx = i * block_size\n",
    "    end_idx = start_idx + block_size\n",
    "    \n",
    "    x_block = x_test[start_idx:end_idx]\n",
    "    \n",
    "    # Time the prediction\n",
    "    start = time.perf_counter()\n",
    "    y_block = chain.predict(x_block)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    y_realtime.append(y_block)\n",
    "    processing_times.append(elapsed)\n",
    "\n",
    "y_realtime = np.concatenate(y_realtime)\n",
    "\n",
    "# Analyze real-time performance\n",
    "avg_time = np.mean(processing_times) * 1000  # Convert to ms\n",
    "max_time = np.max(processing_times) * 1000\n",
    "realtime_factor = (block_size / fs * 1000) / avg_time  # How many times faster than real-time\n",
    "\n",
    "print(f\"\\nReal-time performance:\")\n",
    "print(f\"  Average processing time: {avg_time:.3f} ms\")\n",
    "print(f\"  Max processing time: {max_time:.3f} ms\")\n",
    "print(f\"  Block duration: {block_size/fs*1000:.2f} ms\")\n",
    "print(f\"  Real-time factor: {realtime_factor:.1f}× (higher is better)\")\n",
    "\n",
    "if realtime_factor > 1:\n",
    "    print(f\"  ✅ Real-time capable! ({realtime_factor:.1f}× faster than real-time)\")\n",
    "else:\n",
    "    print(f\"  ❌ Not real-time (too slow by {1/realtime_factor:.1f}×)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Simulated a complete acoustic chain** (instrument + room)\n",
    "2. **Modeled the system using AcousticChain** (cascaded Volterra)\n",
    "3. **Evaluated model performance** on test data\n",
    "4. **Analyzed learned components** (instrument kernels and room IR)\n",
    "5. **Demonstrated real-time processing** capability\n",
    "\n",
    "### Key takeaways:\n",
    "- **Cascaded Volterra systems** can model complex multi-stage audio chains\n",
    "- **Joint identification** learns both nonlinear instrument and linear room\n",
    "- **Real-time processing** is feasible with efficient implementation\n",
    "- **Spectral fidelity**: Model captures harmonics and reverb accurately\n",
    "\n",
    "### Practical applications:\n",
    "1. **Digital audio effects**:\n",
    "   - Amp simulators (guitar, bass)\n",
    "   - Vintage gear emulation (tape saturation, tube warmth)\n",
    "   - Pedal modeling (distortion, overdrive)\n",
    "\n",
    "2. **Virtual acoustics**:\n",
    "   - Concert hall simulation\n",
    "   - Studio monitoring emulation\n",
    "   - Headphone spatialization\n",
    "\n",
    "3. **System identification**:\n",
    "   - Loudspeaker characterization\n",
    "   - Microphone calibration\n",
    "   - Audio codec modeling\n",
    "\n",
    "4. **Audio forensics**:\n",
    "   - Device fingerprinting\n",
    "   - Room acoustics analysis\n",
    "   - Source separation\n",
    "\n",
    "### Extensions:\n",
    "- **Multiple instruments**: Use MIMO TT-Volterra for multi-microphone setups\n",
    "- **Time-varying systems**: Track parameter drift with RLS/adaptive filtering\n",
    "- **Higher-order models**: Capture more complex nonlinearities (N > 3)\n",
    "- **GPU acceleration**: Use JAX/PyTorch for real-time on embedded devices\n",
    "\n",
    "### Model deployment:\n",
    "```python\n",
    "# Save trained model\n",
    "import pickle\n",
    "with open('guitar_amp_chain.pkl', 'wb') as f:\n",
    "    pickle.dump(chain, f)\n",
    "\n",
    "# Load and use in production\n",
    "with open('guitar_amp_chain.pkl', 'rb') as f:\n",
    "    chain_loaded = pickle.load(f)\n",
    "\n",
    "# Process audio stream\n",
    "y_processed = chain_loaded.predict(x_stream)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This completes the notebook series! You now have a complete toolkit for:\n",
    "- Memory Polynomial (MP) modeling\n",
    "- Generalized Memory Polynomial (GMP) with cross-terms\n",
    "- Tensor-Train Volterra for full MIMO systems\n",
    "- Automatic model selection\n",
    "- Real-world acoustic chain applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
