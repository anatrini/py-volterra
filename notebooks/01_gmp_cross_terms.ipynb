{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Memory Polynomial (GMP): Cross-Memory Terms\n",
    "\n",
    "This notebook demonstrates **Generalized Memory Polynomial (GMP)** models, which extend the standard Memory Polynomial by adding **cross-memory interaction terms**.\n",
    "\n",
    "## Why GMP?\n",
    "\n",
    "Standard Memory Polynomial (MP) models use only **diagonal terms**:\n",
    "$$\n",
    "y(t) = \\sum_{n=1}^{N} \\sum_{m=0}^{M-1} h_n[m] \\cdot x^n(t-m)\n",
    "$$\n",
    "\n",
    "But real systems often exhibit **cross-memory effects**, such as:\n",
    "- $x(t) \\cdot x(t-1)$ — interaction between current and past inputs\n",
    "- $x^2(t) \\cdot x(t-3)$ — mixed-order memory coupling\n",
    "\n",
    "GMP adds these cross-terms selectively:\n",
    "$$\n",
    "y(t) = \\underbrace{\\sum_{n,m} h_n[m] \\cdot x^n(t-m)}_{\\text{Diagonal (MP)}} + \\underbrace{\\sum_{(n,m) \\in \\mathcal{L}} g_{n,m} \\cdot x^n(t-m)}_{\\text{Cross-terms (GMP)}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}$ defines the **lag structure** (which cross-terms to include).\n",
    "\n",
    "**Applications:**\n",
    "- Power amplifier modeling (RF/microwave engineering)\n",
    "- Audio distortion with intermodulation products\n",
    "- Biomedical signal processing (EEG, EMG)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "from volterra import GeneralizedMemoryPolynomial\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generate Data with Cross-Memory Effects\n",
    "\n",
    "We'll create a system with **explicit cross-memory interaction**:\n",
    "$$\n",
    "y(t) = 0.8 x(t) + 0.1 x^2(t) + \\underbrace{0.15 \\cdot x(t) \\cdot x(t-2)}_{\\text{Cross-memory!}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate input signal\n",
    "fs = 48000\n",
    "duration = 0.5\n",
    "n_samples = int(fs * duration)\n",
    "\n",
    "# Bandlimited noise\n",
    "x_white = np.random.randn(n_samples)\n",
    "sos = signal.butter(6, [200, 6000], btype='bandpass', fs=fs, output='sos')\n",
    "x = signal.sosfilt(sos, x_white)\n",
    "x = x / np.std(x) * 0.3\n",
    "\n",
    "# Create delayed version for cross-term\n",
    "x_delayed = np.concatenate([np.zeros(2), x[:-2]])  # x(t-2)\n",
    "\n",
    "# True system with cross-memory interaction\n",
    "y_nonlinear = (\n",
    "    0.8 * x +                    # Linear term\n",
    "    0.1 * x**2 +                 # Quadratic (diagonal)\n",
    "    0.15 * x * x_delayed         # CROSS-MEMORY INTERACTION\n",
    ")\n",
    "\n",
    "# Add memory via IIR filter\n",
    "b = [0.2, -0.38, 0.18]\n",
    "a = [1.0, -1.9, 0.94]\n",
    "y_clean = signal.lfilter(b, a, y_nonlinear)\n",
    "\n",
    "# Add noise\n",
    "noise = np.random.randn(n_samples) * 0.01\n",
    "y = y_clean + noise\n",
    "\n",
    "print(f\"Generated {n_samples} samples with cross-memory interaction\")\n",
    "print(f\"Cross-term contribution: x(t) * x(t-2)\")\n",
    "print(f\"SNR: {10 * np.log10(np.mean(y_clean**2) / np.mean(noise**2)):.1f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-memory effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Compare MP-only vs. full signal (with cross-term)\n",
    "y_mp_only = signal.lfilter(b, a, 0.8 * x + 0.1 * x**2)  # Without cross-term\n",
    "y_with_cross = y_clean  # With cross-term\n",
    "\n",
    "t_ms = np.arange(2000) / fs * 1000\n",
    "axes[0].plot(t_ms, y_mp_only[:2000], label='MP only (no cross-term)', alpha=0.7)\n",
    "axes[0].plot(t_ms, y_with_cross[:2000], label='With cross-memory', alpha=0.7)\n",
    "axes[0].set_xlabel('Time (ms)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].set_title('Impact of Cross-Memory Term')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Difference (what the cross-term adds)\n",
    "cross_contribution = y_with_cross - y_mp_only\n",
    "axes[1].plot(t_ms, cross_contribution[:2000], color='red', alpha=0.7)\n",
    "axes[1].axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1].set_xlabel('Time (ms)')\n",
    "axes[1].set_ylabel('Amplitude')\n",
    "axes[1].set_title('Cross-Memory Contribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"RMS of cross-term contribution: {np.sqrt(np.mean(cross_contribution**2)):.4f}\")\n",
    "print(f\"RMS of full signal: {np.sqrt(np.mean(y_with_cross**2)):.4f}\")\n",
    "print(f\"Cross-term represents {100 * np.sqrt(np.mean(cross_contribution**2)) / np.sqrt(np.mean(y_with_cross**2)):.1f}% of signal energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Compare MP vs. GMP\n",
    "\n",
    "Let's fit both models and see which one captures the cross-memory effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "n_train = int(0.7 * n_samples)\n",
    "x_train, x_test = x[:n_train], x[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit standard Memory Polynomial (diagonal only)\n",
    "mp_model = GeneralizedMemoryPolynomial(\n",
    "    memory_length=10,\n",
    "    order=3,\n",
    "    lags=None,  # None = diagonal MP\n",
    "    lambda_reg=1e-6\n",
    ")\n",
    "mp_model.fit(x_train, y_train)\n",
    "\n",
    "# Fit Generalized Memory Polynomial (with cross-terms)\n",
    "# Custom lag structure: include cross-lags for order 1\n",
    "gmp_lags = {\n",
    "    1: [0, 1, 2, 3, 4, 5],  # Linear: include lags 0-5 (cross-terms!)\n",
    "    2: [0, 1, 2],           # Quadratic: lags 0-2\n",
    "    3: [0, 1]               # Cubic: lags 0-1\n",
    "}\n",
    "\n",
    "gmp_model = GeneralizedMemoryPolynomial(\n",
    "    memory_length=10,\n",
    "    order=3,\n",
    "    lags=gmp_lags,  # Custom lag structure\n",
    "    lambda_reg=1e-6\n",
    ")\n",
    "gmp_model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"  MP (diagonal):  {mp_model.coeffs_.size} parameters\")\n",
    "print(f\"  GMP (with cross-terms): {gmp_model.coeffs_.size} parameters\")\n",
    "print(f\"\\nGMP adds {gmp_model.coeffs_.size - mp_model.coeffs_.size} extra parameters for cross-terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_mp_pred = mp_model.predict(x_test)\n",
    "y_gmp_pred = gmp_model.predict(x_test)\n",
    "\n",
    "# Trim ground truth\n",
    "M = 10\n",
    "y_test_trimmed = y_test[M - 1:]\n",
    "\n",
    "# Compute NMSE\n",
    "def compute_nmse(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    signal_power = np.mean(y_true ** 2)\n",
    "    nmse_db = 10 * np.log10(mse / signal_power)\n",
    "    return nmse_db\n",
    "\n",
    "nmse_mp = compute_nmse(y_test_trimmed, y_mp_pred)\n",
    "nmse_gmp = compute_nmse(y_test_trimmed, y_gmp_pred)\n",
    "\n",
    "print(\"Test NMSE:\")\n",
    "print(f\"  MP (diagonal):        {nmse_mp:.2f} dB\")\n",
    "print(f\"  GMP (with cross-terms): {nmse_gmp:.2f} dB\")\n",
    "print(f\"\\nImprovement: {nmse_mp - nmse_gmp:.2f} dB\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if nmse_gmp < nmse_mp - 3:\n",
    "    print(\"  ✅ GMP significantly outperforms MP → cross-memory effects are present!\")\n",
    "elif nmse_gmp < nmse_mp - 1:\n",
    "    print(\"  ⚠️  GMP slightly better → weak cross-memory effects\")\n",
    "else:\n",
    "    print(\"  ❌ No improvement → system is likely diagonal (use MP to avoid overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "n_plot = 1000\n",
    "t_ms = np.arange(n_plot) / fs * 1000\n",
    "\n",
    "# MP prediction\n",
    "axes[0, 0].plot(t_ms, y_test_trimmed[:n_plot], label='True', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 0].plot(t_ms, y_mp_pred[:n_plot], label='MP', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[0, 0].set_xlabel('Time (ms)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].set_title(f'Memory Polynomial (NMSE: {nmse_mp:.2f} dB)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# GMP prediction\n",
    "axes[0, 1].plot(t_ms, y_test_trimmed[:n_plot], label='True', alpha=0.7, linewidth=1.5)\n",
    "axes[0, 1].plot(t_ms, y_gmp_pred[:n_plot], label='GMP', alpha=0.7, linewidth=1.5, linestyle='--')\n",
    "axes[0, 1].set_xlabel('Time (ms)')\n",
    "axes[0, 1].set_ylabel('Amplitude')\n",
    "axes[0, 1].set_title(f'Generalized MP (NMSE: {nmse_gmp:.2f} dB)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error comparison\n",
    "error_mp = y_test_trimmed[:n_plot] - y_mp_pred[:n_plot]\n",
    "error_gmp = y_test_trimmed[:n_plot] - y_gmp_pred[:n_plot]\n",
    "\n",
    "axes[1, 0].plot(t_ms, error_mp, alpha=0.7, color='blue', label='MP error')\n",
    "axes[1, 0].plot(t_ms, error_gmp, alpha=0.7, color='green', label='GMP error')\n",
    "axes[1, 0].axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "axes[1, 0].set_xlabel('Time (ms)')\n",
    "axes[1, 0].set_ylabel('Prediction Error')\n",
    "axes[1, 0].set_title('Error Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 1].hist(error_mp, bins=50, density=True, alpha=0.5, label='MP', color='blue')\n",
    "axes[1, 1].hist(error_gmp, bins=50, density=True, alpha=0.5, label='GMP', color='green')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Prediction Error')\n",
    "axes[1, 1].set_ylabel('Probability Density')\n",
    "axes[1, 1].set_title(f'Error Distribution (MP σ={np.std(error_mp):.4f}, GMP σ={np.std(error_gmp):.4f})')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding GMP Lag Structures\n",
    "\n",
    "GMP allows flexible lag structures. Let's visualize different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lag structures\n",
    "lag_configs = [\n",
    "    {\n",
    "        'name': 'Diagonal (MP)',\n",
    "        'lags': None  # Default: diagonal\n",
    "    },\n",
    "    {\n",
    "        'name': 'GMP Light',\n",
    "        'lags': {1: [0, 1, 2], 2: [0, 1], 3: [0]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'GMP Medium',\n",
    "        'lags': {1: [0, 1, 2, 3, 4], 2: [0, 1, 2], 3: [0, 1]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'GMP Heavy',\n",
    "        'lags': {1: [0, 1, 2, 3, 4, 5, 6], 2: [0, 1, 2, 3], 3: [0, 1, 2]}\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in lag_configs:\n",
    "    model = GeneralizedMemoryPolynomial(\n",
    "        memory_length=10,\n",
    "        order=3,\n",
    "        lags=config['lags'],\n",
    "        lambda_reg=1e-6\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    nmse = compute_nmse(y_test_trimmed, y_pred)\n",
    "    n_params = model.coeffs_.size\n",
    "    \n",
    "    results.append({\n",
    "        'name': config['name'],\n",
    "        'n_params': n_params,\n",
    "        'nmse_db': nmse\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:15s}: {n_params:3d} params, NMSE = {nmse:6.2f} dB\")\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "names = [r['name'] for r in results]\n",
    "params = [r['n_params'] for r in results]\n",
    "nmses = [r['nmse_db'] for r in results]\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "bars = ax.bar(names, nmses, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Annotate with parameter counts\n",
    "for i, (bar, n_param) in enumerate(zip(bars, params)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height - 1,\n",
    "            f'{n_param} params',\n",
    "            ha='center', va='top', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test NMSE (dB)', fontsize=12)\n",
    "ax.set_title('Impact of Lag Structure on GMP Performance', fontsize=14, fontweight='bold')\n",
    "ax.axhline(-20, color='green', linestyle='--', linewidth=1.5, alpha=0.5, label='Excellent threshold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: More lags ≠ always better!\")\n",
    "print(\"  - Too few lags: underfitting (can't capture cross-memory)\")\n",
    "print(\"  - Too many lags: overfitting risk + increased computational cost\")\n",
    "print(\"  - Use cross-validation or model selection to choose optimal structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Coefficient Analysis: What Did GMP Learn?\n",
    "\n",
    "Let's examine the learned coefficients to understand which cross-terms are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GMP coefficients\n",
    "gmp_coeffs = gmp_model.coeffs_\n",
    "\n",
    "print(f\"GMP coefficient matrix shape: {gmp_coeffs.shape}\")\n",
    "print(f\"Total nonzero coefficients: {np.count_nonzero(gmp_coeffs)}\")\n",
    "\n",
    "# Visualize coefficient structure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for order_idx in range(3):\n",
    "    order = order_idx + 1\n",
    "    coeffs_order = gmp_coeffs[:, order_idx]\n",
    "    \n",
    "    # Find which lags are active (nonzero)\n",
    "    active_lags = np.where(coeffs_order != 0)[0]\n",
    "    active_values = coeffs_order[active_lags]\n",
    "    \n",
    "    axes[order_idx].stem(active_lags, active_values, basefmt=' ')\n",
    "    axes[order_idx].set_xlabel('Memory Lag')\n",
    "    axes[order_idx].set_ylabel(f'Coefficient h_{order}[m]')\n",
    "    axes[order_idx].set_title(f'Order {order} Coefficients ({len(active_lags)} active lags)')\n",
    "    axes[order_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight the most important lag\n",
    "    if len(active_values) > 0:\n",
    "        max_idx = np.argmax(np.abs(active_values))\n",
    "        max_lag = active_lags[max_idx]\n",
    "        axes[order_idx].axvline(max_lag, color='red', linestyle='--', \n",
    "                                linewidth=2, alpha=0.5, label=f'Strongest: lag {max_lag}')\n",
    "        axes[order_idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify dominant cross-lags\n",
    "print(\"\\nDominant cross-lags (lags with largest absolute coefficients):\")\n",
    "for order_idx in range(3):\n",
    "    order = order_idx + 1\n",
    "    coeffs_order = gmp_coeffs[:, order_idx]\n",
    "    active_lags = np.where(coeffs_order != 0)[0]\n",
    "    \n",
    "    if len(active_lags) > 0:\n",
    "        # Sort by absolute value\n",
    "        sorted_idx = np.argsort(np.abs(coeffs_order[active_lags]))[::-1]\n",
    "        top_lags = active_lags[sorted_idx[:3]]  # Top 3\n",
    "        top_values = coeffs_order[top_lags]\n",
    "        \n",
    "        print(f\"  Order {order}:\")\n",
    "        for lag, val in zip(top_lags, top_values):\n",
    "            print(f\"    Lag {lag}: {val:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Generated data with cross-memory effects** (interaction between $x(t)$ and $x(t-2)$)\n",
    "2. **Compared MP vs. GMP models** and showed GMP significantly outperforms MP\n",
    "3. **Explored different lag structures** and their impact on performance\n",
    "4. **Analyzed learned coefficients** to identify dominant cross-lags\n",
    "\n",
    "### When to use GMP:\n",
    "- ✅ **Cross-memory effects suspected** (e.g., intermodulation distortion)\n",
    "- ✅ **MP underfits** (poor NMSE despite sufficient memory_length/order)\n",
    "- ✅ **Domain knowledge** suggests specific lag interactions\n",
    "- ❌ **No cross-memory** → stick with MP (simpler, less overfitting risk)\n",
    "- ❌ **High-dimensional MIMO** → use TT-Volterra instead\n",
    "\n",
    "### Practical tips:\n",
    "1. **Start with MP** (diagonal) as baseline\n",
    "2. **Add cross-lags incrementally** if MP underfits\n",
    "3. **Use regularization** (`lambda_reg > 0`) to prevent overfitting\n",
    "4. **Cross-validate** lag structure selection\n",
    "5. **Use ModelSelector** (Notebook 03) for automatic MP vs. GMP selection\n",
    "\n",
    "### Next steps:\n",
    "- **Notebook 02**: Tensor-Train Volterra for full MIMO systems\n",
    "- **Notebook 03**: Automatic model selection (MP vs GMP vs TT-Full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
